{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Finetunine of Falcon 7B model on Norewegian Dialogue Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/lars/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/lars/miniconda3/envs/llm/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /home/lars/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lars/miniconda3/envs/llm/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/lars/miniconda3/envs/llm/lib/libcudart.so'), PosixPath('/home/lars/miniconda3/envs/llm/lib/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "## Inference\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "device_map = {\"\": 0}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Guanaco Instruction Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/lars/.cache/huggingface/datasets/timdettmers___json/timdettmers--openassistant-guanaco-c93588435bc90172/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"timdettmers/openassistant-guanaco\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset - Norwegian Dialogue Generation Disclaimer this dataset was generated using GPT4 and should only be used for non commercial purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/lars/.cache/huggingface/datasets/json/default-b55761bd55293d82/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "Found cached dataset json (/home/lars/.cache/huggingface/datasets/json/default-764d7df3d9be3714/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'text': '### Human Please generate a dialogue between Erik and Kristin, two Norwegians in their forties discussing an upcoming reunion party. Keep it colloquial. ### Assistant Erik: Hei Kristin, har du hørt om gjenforeningsfesten som skal arrangeres neste måned?\\nKristin: Hei Erik! Ja, jeg har fått invitasjonen. Det er jo utrolig at det er 20 år siden vi gikk ut av videregående, ikke sant? Tiden flyr ass!\\nErik: Absolutt! Det er helt latterlig hvordan tiden har gått. Tenk at vi alle samles igjen etter så lang tid. Planlegger du å gå da?\\nKristin: Ja, jeg har tenkt å dra dit. Det kan være gøy å se alle sammen igjen og høre hva de driver med nå, liksom. Hva med deg?\\nErik: Ja, jeg skal også dit. Jeg er spent på å se hvor mye folk har forandret seg og høre historiene deres. Det blir nok noen gode mimrestunder!\\nKristin: Hehe, det er sikkert! Men, ærlig talt, jeg er litt nervøs også. Jeg har ikke sett noen av dem på så lenge, og jeg er redd for at det kan bli litt kleint.\\nErik: Det skjønner jeg godt. Men jeg tror alle vil være litt nervøse og spent, og vi skal jo dit for å kose oss og ha en god tid sammen. Så jeg tenker at det ordner seg, vet du.\\nKristin: Ja, du har nok rett. Det skal bli gøy uansett, og jeg gleder meg til å catche opp med deg og de andre. Kanskje vi kan møtes før festen, så vi kan gå sammen?\\nErik: Ja, det høres ut som en god idé! Da kan vi jo også oppdatere hverandre litt om hva som har skjedd i livene våre før vi møter resten av gjengen.\\nKristin: Flott! Da avtaler vi å møtes et par timer før festen, og så ta en kaffe og slappe av litt. Gleder meg!\\nErik: Gleder meg også, Kristin'}\n",
      "{'id': '0', 'text': '### Human Please generate a dialogue between Lisbeth, a young single mother, and Eirik, a restaurant owner discussing their week. Keep it colloquial. ### Assistant Lisbeth: Hei Eirik, hvordan har uka di vært?\\nEirik: Hei Lisbeth! Uka har vært hektisk, altså. Vi har hatt mye å gjøre på restauranten. Og du da, hvordan har uka di vært med den lille?\\nLisbeth: Åh, vet du, det har vært både oppturer og nedturer. Jeg elsker jo å være mamma, men det kan være litt slitsomt å gjøre alt på egenhånd, ass.\\nEirik: Jeg kan tenke meg det, men du er jo en supermamma, så jeg er sikker på at du takler det bra.\\nLisbeth: Åh, takk Eirik, det betyr mye å høre det! Så, hvordan går det med restauranten da? Har du noen nye retter på menyen?\\nEirik: Ja, faktisk! Vi har lagt til noen spennende vegetarretter og en ny dessert. Det ser ut til at kundene elsker det, så jeg er fornøyd.\\nLisbeth: Det høres kjempegodt ut! Jeg må ta med meg datteren min og prøve de nye rettene en dag.\\nEirik: Absolutt, det hadde vært koselig! Si ifra når det passer for deg, så reserverer jeg et bord til dere.\\nLisbeth: Tusen takk, Eirik! Det setter jeg stor pris på. Ha en fin uke videre, og vi snakkes snart!\\nEirik: Takk, Lisbeth, og ha en super uke selv! Vi snakkes!'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the transformed data into a HuggingFace Dataset\n",
    "dataset = load_dataset('json', data_files='transformed.json', split='train')\n",
    "validation_dataset = load_dataset('json', data_files='transformed_valdiation.json', split='train')\n",
    "# Let's check the first element in the dataset\n",
    "print(dataset[0])\n",
    "print(validation_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b0adad4ea84953b84605d3ac4189c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n",
    "\n",
    "model_name = \"tiiuae/falcon-7b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    quantization_config=bnb_config, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finetune\n",
    "\n",
    "from peft import LoraConfig\n",
    "\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 64\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"query_key_value\",\n",
    "        \"dense\",\n",
    "        \"dense_h_to_4h\",\n",
    "        \"dense_4h_to_h\",\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = \"./results\"\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "optim = \"paged_adamw_32bit\"\n",
    "save_steps = 10\n",
    "logging_steps = 10\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.3\n",
    "max_steps = 500\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"constant\"\n",
    "eval_steps = 10\n",
    "do_eval = True\n",
    "evaluation_strategy = \"steps\"\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    eval_steps=eval_steps,\n",
    "    do_eval=do_eval,\n",
    "    evaluation_strategy=evaluation_strategy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lars/miniconda3/envs/llm/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "Loading cached processed dataset at /home/lars/.cache/huggingface/datasets/json/default-b55761bd55293d82/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bcbb27c603de76b4.arrow\n",
      "Loading cached processed dataset at /home/lars/.cache/huggingface/datasets/json/default-764d7df3d9be3714/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-391ac2f8062796bd.arrow\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 512\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune Model - Beware! If your dataset is small it will overfit super quickly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 13/500 01:35 < 1:10:42, 0.11 it/s, Epoch 1.37/63]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.882900</td>\n",
       "      <td>1.776837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1642\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1643\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1644\u001b[0m )\n\u001b[0;32m-> 1645\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1646\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1647\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1648\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1649\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1650\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1935\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1937\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1938\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1940\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1941\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1942\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1943\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1944\u001b[0m ):\n\u001b[1;32m   1945\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1946\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/trainer.py:2770\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2768\u001b[0m         scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m   2769\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2770\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[1;32m   2772\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/accelerate/accelerator.py:1834\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1832\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1833\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1834\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1835\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1836\u001b[0m     loss\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load vanilla model and run inference for crude comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697abfd4a744433fbbc7aaa694f17c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"tiiuae/falcon-7b\", quantization_config=bnb_config, device_map=device_map, trust_remote_code=True,\n",
    "    load_in_4bit=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lars/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#### Human: Write a short dialogue between two Norwegian people, Cristiana and Waleed, who just met. Keep it colloquial.### Assitant: Hei, Cristiana! Hvordan går det med deg? Det er lenge siden sist vi så hverandre.\\n\\nCristiana: Hei, Waleed! Jo, det er virkelig for lenge siden. Tja, livet har vært ganske hektisk, men bra igjen. Hvordan står det til med deg?\\n\\nWaleed: Åh, du sier noe der! Jeg har så mye å gjøre med jobbintervjuer og sånt, jeg har jo ikke fått en jobb ennå.\\n\\nCristiana: Hmmmm, det er jo ikke alltid så bra når man ikke har jobb. Men det er jo ikke så lett å']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#### Human: Write a short dialogue between two Norwegian people, Cristiana and Waleed, who just met. Keep it colloquial.### Assitant: Hei, Cristiana! Hvordan går det med deg? Det er lenge siden sist vi så hverandre.\\n\\nCristiana: Hei, Waleed! Jo, det er virkelig for lenge siden. Tja, livet har vært ganske hektisk, men bra igjen. Hvordan står det til med deg?\\n\\nWaleed: Åh, du sier noe der! Jeg har så mye å gjøre med jobbintervjuer og sånt, jeg har jo ikke fått en jobb ennå.\\n\\nCristiana: Ja, det er jo ikke alltid lett å finne en jobb. Hvordan går det med dine studier?\\n\\nW']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#### Human: Write a short dialogue between two Norwegian people, Cristiana and Waleed, who just met. Keep it colloquial.### Assitant: Hei, Cristiana! Hvordan går det med deg? Det er lenge siden sist vi så hverandre.\\n\\nCristiana: Hei, Waleed! Jo, det er virkelig for lenge siden. Tja, livet har vært ganske hektisk, men bra igjen. Hvordan står det til med deg?\\n\\nWaleed: Åh, du sier noe der! Jeg har så mye å gjøre med jobbintervjuer og sånt, jeg har jo ikke fått en jobb ennå.\\n\\nCristiana: Hm, det er jo veldig vanskelig. Har du ikke lest meg?\\n\\nWaleed: Ja, jeg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#### Human: Write a short dialogue between two Norwegian people, Cristiana and Waleed, who just met. Keep it colloquial.### Assitant: Hei, Cristiana! Hvordan går det med deg? Det er lenge siden sist vi så hverandre.\\n\\nCristiana: Hei, Waleed! Jo, det er virkelig for lenge siden. Tja, livet har vært ganske hektisk, men bra igjen. Hvordan står det til med deg?\\n\\nWaleed: Åh, du sier noe der! Jeg har så mye å gjøre med jobbintervjuer og sånt, jeg har jo ikke fått en jobb ennå.\\n\\nCristiana: Ja, jeg har ikke hatt noe jobb i lenge. Jeg har vært på jobbe i Roma og i']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#### Human: Write a short dialogue between two Norwegian people, Cristiana and Waleed, who just met. Keep it colloquial.### Assitant: Hei, Cristiana! Hvordan går det med deg? Det er lenge siden sist vi så hverandre.\\n\\nCristiana: Hei, Waleed! Jo, det er virkelig for lenge siden. Tja, livet har vært ganske hektisk, men bra igjen. Hvordan står det til med deg?\\n\\nWaleed: Åh, du sier noe der! Jeg har så mye å gjøre med jobbintervjuer og sånt, jeg har jo ikke fått en jobb ennå.\\n\\nCristiana: Ja, det er jo ikke så lett i dag. Men hva skal man gjøre?\\n\\nWaleed: Jeg']\n",
      "['#### Human: Write a short dialogue between two Norwegian people, Cristiana and Waleed, who just met. Keep it colloquial.### Assitant: Hei, Cristiana! Hvordan går det med deg? Det er lenge siden sist vi så hverandre.\\n\\nCristiana: Hei, Waleed! Jo, det er virkelig for lenge siden. Tja, livet har vært ganske hektisk, men bra igjen. Hvordan står det til med deg?\\n\\nWaleed: Åh, du sier noe der! Jeg har så mye å gjøre med jobbintervjuer og sånt, jeg har jo ikke fått en jobb ennå.\\n\\nCristiana: Ja, det er jo ikke så lett å finne jobb i dag. Men du har jo en veldig flott lille job']\n",
      "3.13 s ± 8.89 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 5\n",
    "### Inference\n",
    "\n",
    "# inference test\n",
    "with torch.inference_mode():\n",
    "    text = '#### Human: Write a short dialogue between two Norwegian people, Cristiana and Waleed, who just met. Keep it colloquial.### Assitant: Hei, Cristiana! Hvordan går det med deg? Det er lenge siden sist vi så hverandre.\\n\\nCristiana: Hei, Waleed! Jo, det er virkelig for lenge siden. Tja, livet har vært ganske hektisk, men bra igjen. Hvordan står det til med deg?\\n\\nWaleed: Åh, du sier noe der! Jeg har så mye å gjøre med jobbintervjuer og sånt, jeg har jo ikke fått en jobb ennå.\\n\\nCristiana:'\n",
    "    outputs = model.generate(input_ids=tokenizer(text, return_tensors=\"pt\").input_ids.to(model.device), \n",
    "                            max_new_tokens=30, \n",
    "                            temperature=0.5, \n",
    "                            num_return_sequences=1,\n",
    "                            do_sample=True,\n",
    "                            use_cache = True)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load finetuned model and run inference for crude comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbaf5dd7fab24af99f311c4d147a8f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Load finetuned model \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"tiiuae/falcon-7b\", quantization_config=bnb_config, device_map=device_map, trust_remote_code=True,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False)\n",
    "# change this path to your checkpoint folder path\n",
    "model_id = \"/home/lars/ML/LLM/results/checkpoint-30\"\n",
    "model = PeftModel.from_pretrained(model, model_id)\n",
    "model.config.use_cache = False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#### Human: Write a short dialogue between two Norwegian people, Cristiana and Waleed, who just met. Keep it colloquial.### Assitant: Hei, Cristiana! Hvordan går det med deg? Det er lenge siden sist vi så hverandre.\\n\\nCristiana: Hei, Waleed! Jo, det er virkelig for lenge siden. Tja, livet har vært ganske hektisk, men bra igjen. Hvordan står det til med deg?\\n\\nWaleed: Åh, du sier noe der! Jeg har så mye å gjøre med jobbintervjuer og sånt, jeg har jo ikke fått en jobb ennå.\\n\\nCristiana: Åh, det høres ikke bra ut. Har du prøvd å jobbe med sosiale medier eller noen annen']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#### Human: Write a short dialogue between two Norwegian people, Cristiana and Waleed, who just met. Keep it colloquial.### Assitant: Hei, Cristiana! Hvordan går det med deg? Det er lenge siden sist vi så hverandre.\\n\\nCristiana: Hei, Waleed! Jo, det er virkelig for lenge siden. Tja, livet har vært ganske hektisk, men bra igjen. Hvordan står det til med deg?\\n\\nWaleed: Åh, du sier noe der! Jeg har så mye å gjøre med jobbintervjuer og sånt, jeg har jo ikke fått en jobb ennå.\\n\\nCristiana: Åh, det er så vanskelig med jobbmarkeder nå. Men du er så flink, jeg er sikker på']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#### Human: Write a short dialogue between two Norwegian people, Cristiana and Waleed, who just met. Keep it colloquial.### Assitant: Hei, Cristiana! Hvordan går det med deg? Det er lenge siden sist vi så hverandre.\\n\\nCristiana: Hei, Waleed! Jo, det er virkelig for lenge siden. Tja, livet har vært ganske hektisk, men bra igjen. Hvordan står det til med deg?\\n\\nWaleed: Åh, du sier noe der! Jeg har så mye å gjøre med jobbintervjuer og sånt, jeg har jo ikke fått en jobb ennå.\\n\\nCristiana: Åh, det er så vanskelig å høre. Har du lagt noen annonser eller gjort en søkn']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#### Human: Write a short dialogue between two Norwegian people, Cristiana and Waleed, who just met. Keep it colloquial.### Assitant: Hei, Cristiana! Hvordan går det med deg? Det er lenge siden sist vi så hverandre.\\n\\nCristiana: Hei, Waleed! Jo, det er virkelig for lenge siden. Tja, livet har vært ganske hektisk, men bra igjen. Hvordan står det til med deg?\\n\\nWaleed: Åh, du sier noe der! Jeg har så mye å gjøre med jobbintervjuer og sånt, jeg har jo ikke fått en jobb ennå.\\n\\nCristiana: Åh, det er så vanskelig. Hva med å finne en god jobb? Vi kan være en team og hj']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#### Human: Write a short dialogue between two Norwegian people, Cristiana and Waleed, who just met. Keep it colloquial.### Assitant: Hei, Cristiana! Hvordan går det med deg? Det er lenge siden sist vi så hverandre.\\n\\nCristiana: Hei, Waleed! Jo, det er virkelig for lenge siden. Tja, livet har vært ganske hektisk, men bra igjen. Hvordan står det til med deg?\\n\\nWaleed: Åh, du sier noe der! Jeg har så mye å gjøre med jobbintervjuer og sånt, jeg har jo ikke fått en jobb ennå.\\n\\nCristiana: Åh, det er skjønt. Men du skal nok finne noe. Hva med å ta en kaffe s']\n",
      "['#### Human: Write a short dialogue between two Norwegian people, Cristiana and Waleed, who just met. Keep it colloquial.### Assitant: Hei, Cristiana! Hvordan går det med deg? Det er lenge siden sist vi så hverandre.\\n\\nCristiana: Hei, Waleed! Jo, det er virkelig for lenge siden. Tja, livet har vært ganske hektisk, men bra igjen. Hvordan står det til med deg?\\n\\nWaleed: Åh, du sier noe der! Jeg har så mye å gjøre med jobbintervjuer og sånt, jeg har jo ikke fått en jobb ennå.\\n\\nCristiana: Åh, det er vanskelig. Hva med å prøve etter et annet jobb, kanskje? Jeg']\n",
      "3.79 s ± 12.8 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 5\n",
    "### Inference\n",
    "\n",
    "# inference test\n",
    "with torch.inference_mode():\n",
    "    text = '#### Human: Write a short dialogue between two Norwegian people, Cristiana and Waleed, who just met. Keep it colloquial.### Assitant: Hei, Cristiana! Hvordan går det med deg? Det er lenge siden sist vi så hverandre.\\n\\nCristiana: Hei, Waleed! Jo, det er virkelig for lenge siden. Tja, livet har vært ganske hektisk, men bra igjen. Hvordan står det til med deg?\\n\\nWaleed: Åh, du sier noe der! Jeg har så mye å gjøre med jobbintervjuer og sånt, jeg har jo ikke fått en jobb ennå.\\n\\nCristiana:'\n",
    "    outputs = model.generate(input_ids=tokenizer(text, return_tensors=\"pt\").input_ids.to(model.device), \n",
    "                            max_new_tokens=30, \n",
    "                            temperature=0.5, \n",
    "                            num_return_sequences=1,\n",
    "                            do_sample=True,\n",
    "                            use_cache = True)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
